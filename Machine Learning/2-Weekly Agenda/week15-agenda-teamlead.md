<h1><strong><span style="color: #77C8D5;">Week-15 (Oct 13th - Oct 19th)</strong></span>

![logo](ds_agenda_logo.png)

<br>


<h1><strong><span style="color: #3498DB;">Meeting Agenda</strong></h1></span>

<span class="c16 c30">▶ </span><span
class="c42 c82">Icebreaking (10m)</span><span class="c16 c23"> </span>

<span class="c16 c30">▶ </span><span
class="c42 c82">Micro Learning & Presentations (30m)</span><span class="c46 c42 c48"> </span>


<span class="c30">▶ </span><span class="c46 c48 c42">Project Discussion (40m)</span>

<br>
<br>
<br>

<div style="page-break-after: always;"></div>

<h1><strong><span style="color: #3498DB;">Teamwork Schedule</strong></h1></span>

<table style= "width:100%;">
                <tr>
                <td style="color: #FA8072; text-align:left "><h3><strong><p>Ice-breaking</td>
                <td style="color: #FA8072; text-align:right;"><h3><strong><p>10m</p><td>                </tr>
</table>

- Personal Questions (Stay at home & Corona, Study Environment, Kids etc.) 
- Any challenges (Classes, Coding, studying, etc.) 
- How you’re studying, you need personal advice? 
- Remember that practice makes perfect. 
- What exactly each student does for the team, if they know each other, if they care for each other, if they follow and talk with each other etc. 

<br>
<br>

<table style= "width:100%;">
                <tr>
                <td style="color: #FA8072; text-align:left "><h3><strong><p>Micro Learning & Presentations</td>
                <td style="color: #FA8072; text-align:right;"><h3><strong><p>50m</p><td>                </tr>
</table>
(The problems and subjects in this part, has been prepared especially for you to equip yourself for the interview process and improve your coding skills.
To get the most from this part, it’s highly recommended to be prepared and present the topics in English.
At first, it can be difficult to present in English and you can read from your notes, no problem.
We strongly advise you to force yourself to present especially the interview questions in English.
However, if you don’t feel you can present in English, you can do any part in Turkish, that’s no problem too.
Please remember; we don’t want to put extra pressure to anyone, it’s totally up to you how you prepare for this section.
The main and only aim of this part is to develop your skills that you need during and after the recruitment process and make you ready for the DS career.)
<br><br>
                  
                  
<h3><strong>Questions & Problems Related to Course Topics</strong></h4>
<hr>
                  
**1. Answer the following questions.** 
                  <br>
                  
- [PCA Questions](https://github.com/clarusway/DS-0821-ML-Module-Students/blob/main/2-Weekly%20Agenda/week15-PCA-questions-teamlead.pdf)

<br>

<h3><strong>Interview Questions</strong></h4>
<hr>

**1. Why do we need Dimensionality Reduction?**

**Answer:**
We use size reduction techniques mainly for two reasons: Visualization: Our brain is most accustomed to thinking in three-dimensional space and has trouble imagining higher order dimensions. We can easily understand the orientation of objects and the relationships between objects in one, two, and three dimensions, but as dimensionality increases, we may not be able to do so easily. Since visualization is an important aspect of heuristic data analysis, we need to reduce its size in our high-dimensional data; at the same time, we must protect relationships and information as much as possible. For this reason, size reduction techniques are used extensively for visualization purposes. The dimensionality curse: In machine learning, we estimate the parameters of our models with the available data we have. If we add many properties to our models, their statistical power may decrease. In other words, the parameters of the model increase exponentially with the number of dimensions (properties) of the data. That's why we resort to size reduction techniques to limit the size in the data. On the other hand, some machine learning algorithms are affected by the multidimensionality curse like regression models.

**2. What is PCA in shortly and tell us about the advantages and disadvantages of the PCA?**

**Answer:**
PCA is a complexity reduction technique that tries to reduce it to a smaller set of components representing most of the information in variables. When we look at the advantages of PCA, we can say that we got rid of the multidimensionality curse and its negative results. This will give us the advantage of running our algorithm faster and reducing the possibility of overfitting. In addition, the fact that the components formed with PCA do not correlate with each other. This is a great advantage in regression problems. One of the important advantages of size reduction techniques such as PCA is visualization. We know that that human perception gets better with visualization. Advantages of the PCA in shortly:

Significant reduction from the multidimensional curse
Eradication of correlated features
Improvement of algorithm performance
Reduction of overfitting possibility
Improvements on visualization
Disadvantages:

First of all principal components (PC) are not as readable and interpretable as original features. Also we have to standardize the data before implementing PCA; otherwise PCA will not be able to find the optimal principal components. In addition, after the PCA we may miss some information as compared to the original list of features.

Disadvantages of the PCA in shortly:

Less interpretable
Loss information
Data standardization is necessary

**3. What are the linkage methods used in hierarchical clustering?**

**Answer:**
Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. This hierarchy of clusters is represented as a tree (or dendrogram).

The Agglomerative Clustering which is widely used of hierarchical clustering, performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together.

The linkage criteria determines the metric used for the merge strategy:

Ward minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.
Maximum or complete linkage uses the maximum distances between all observations of the two sets.
Average linkage uses the average of the distances of each observation of the two sets.
Single linkage uses the minimum of the distances between all observations of the two sets.
                  
                  
**4. What are Recommender Systems? Which recommender system models are mostly used? Briefly explain.**

**Answer:**
Recommendation systems are models created based on users' previous preferences or similar users' preferences. They are widely and successfully used by firms like Netflix, Google, Amazon, YouTube etc. Widely used Recommender System models are Content Based Recommendations and Collaborative Based Recommendations.

In Content Based Recommendations the user will be recommended items similar to the ones the user preferred in the past. For example if you bought sports wear on internet, Content Based Recommendation systems will recommend you similar items of sport wears like running shoes etc.

In Collaborative Based Recommendations the user will be recommended items that are preferred by other people with similar tastes and preferences. For example if you bought and read a scientific books, Collaborative Based Recommendation system search and find people who bought and read scientific books and recommend you the other scientific books that read or bought by similar users with you.

<hr>

<br>


<table style= "width:100%;">
                <tr>
                <td style="color: #FA8072; text-align:left "><h3><strong><p>Project Discussion</td>
                <td style="color: #FA8072; text-align:right;"><h3><strong><p>20m</p><td>                </tr>
                
</table>

Please discuss the following subjects within the group.



- [Assigment]()

